{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPodwJ//6WBdaiHqppp0W3s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejasvi1303/mentalhealthprediction/blob/main/Mental_Health_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk joblib streamlit pyngrok --quiet"
      ],
      "metadata": {
        "id": "-ZAYRl56XIqx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr -y\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3MeYsCGBgp4",
        "outputId": "a4dea4d2-8906-4882-eaa3-5038d40db675"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import lightgbm as lgb\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras import Model\n",
        "# Download necessary NLTK data files\n",
        "# Download necessary NLTK data files\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "file_path = '/content/Combined Data.csv'\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File found at:\", file_path)\n",
        "else:\n",
        "    print(\"File not found.\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"Combined Data.csv\", encoding='latin1', on_bad_lines='skip')\n",
        "df.head(10)\n",
        "df.tail(10)\n",
        "print(df.isnull().sum())\n",
        "df = df.dropna()\n",
        "print(df.isnull().sum())\n",
        "df['status'].nunique()\n",
        "sentiment_counts=df['status'].value_counts()\n",
        "print(sentiment_counts)\n",
        "df['status'].unique()\n",
        "df.shape\n",
        "df.info()\n",
        "# Calculate the length of each statement\n",
        "df['statement_length'] = df['statement'].apply(len)\n",
        "\n",
        "# Display basic statistics of statement lengths\n",
        "print(df['statement_length'].describe())\n",
        "\n",
        "# Taking a sample of the dataframe, e.g., 20,000 rows\n",
        "sample_size = 20000\n",
        "df_sample = df.sample(n=sample_size, random_state=1)\n",
        "# Load the spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "def preprocess_texts(texts):\n",
        "    return [preprocess_text(text) for text in texts]\n",
        "\n",
        "\n",
        "# Split the data into batches for multiprocessing\n",
        "num_cores = cpu_count()\n",
        "df_split = np.array_split(df_sample, num_cores)\n",
        "\n",
        "# Create a multiprocessing Pool\n",
        "with Pool(num_cores) as pool:\n",
        "    # Preprocess the text in parallel\n",
        "    results = pool.map(preprocess_texts, [batch['statement'].tolist() for batch in df_split])\n",
        "\n",
        "# Combine the results\n",
        "df_sample['cleaned_statement'] = [item for sublist in results for item in sublist]\n",
        "\n",
        "# Display the first few rows of the DataFrame to confirm the changes\n",
        "print(df_sample[['statement', 'cleaned_statement']].head())\n",
        "df_sample.head()\n",
        "# Extract features and labels\n",
        "processedtext = df_sample['cleaned_statement']\n",
        "sentiment = df_sample['status']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
        "                                                    test_size=0.05, random_state=0)\n",
        "\n",
        "print(f'X_train size: {len(X_train)}')\n",
        "print(f'X_test size: {len(X_test)}')\n",
        "print(f'y_train size: {len(y_train)}')\n",
        "print(f'y_test size: {len(y_test)}')\n",
        "\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "vectoriser.fit(X_train)\n",
        "print(f'Vectoriser fitted.')\n",
        "print('No. of feature_words: ', len(vectoriser.get_feature_names_out()))\n",
        "\n",
        "X_train = vectoriser.transform(X_train)\n",
        "X_test  = vectoriser.transform(X_test)\n",
        "\n",
        "# Use SMOTE to oversample the training data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=bnb, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Score: {grid_search.best_score_}')\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_bnb = grid_search.best_estimator_\n",
        "best_bnb.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_best_bnb = best_bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Tuned Bernoulli Naive Bayes\")\n",
        "print(classification_report(y_test, y_pred_best_bnb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w6gZkH7IYeL",
        "outputId": "6d11fa59-cc11-4022-bcb1-5f373cca18a6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found at: /content/Combined Data.csv\n",
            "Unnamed: 0      0\n",
            "statement     362\n",
            "status          0\n",
            "dtype: int64\n",
            "Unnamed: 0    0\n",
            "statement     0\n",
            "status        0\n",
            "dtype: int64\n",
            "status\n",
            "Normal                  16343\n",
            "Depression              15404\n",
            "Suicidal                10652\n",
            "Anxiety                  3841\n",
            "Bipolar                  2777\n",
            "Stress                   2587\n",
            "Personality disorder     1077\n",
            "Name: count, dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 52681 entries, 0 to 53042\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  52681 non-null  int64 \n",
            " 1   statement   52681 non-null  object\n",
            " 2   status      52681 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.6+ MB\n",
            "count    52681.000000\n",
            "mean       578.718153\n",
            "std        846.274948\n",
            "min          2.000000\n",
            "25%         80.000000\n",
            "50%        317.000000\n",
            "75%        752.000000\n",
            "max      32759.000000\n",
            "Name: statement_length, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               statement  \\\n",
            "39488  i m and i have bad anxiety debilitating i have...   \n",
            "1480   Jessica starred in the musical \"Legally Blonde...   \n",
            "47948  Im so tired I just dont see a point to my su...   \n",
            "19681  My life 1 year ago was completely different. I...   \n",
            "37629  RT @no_onespecixl: Know one enjoys my company ...   \n",
            "\n",
            "                                       cleaned_statement  \n",
            "39488  bad anxiety debilitating able keep job since w...  \n",
            "1480   jessica starred musical legally blonde elle wo...  \n",
            "47948  im tired dont see point suffering dont unde...  \n",
            "19681  life 1 year ago completely different chick mag...  \n",
            "37629  rt noonespecixl know one enjoys company make e...  \n",
            "X_train size: 19000\n",
            "X_test size: 1000\n",
            "y_train size: 19000\n",
            "y_test size: 1000\n",
            "Vectoriser fitted.\n",
            "No. of feature_words:  500000\n",
            "Best Parameters: {'alpha': 0.1}\n",
            "Best Score: 0.8011026708183809\n",
            "Tuned Bernoulli Naive Bayes\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "             Anxiety       0.86      0.32      0.47        75\n",
            "             Bipolar       1.00      0.09      0.17        55\n",
            "          Depression       0.54      0.52      0.53       284\n",
            "              Normal       0.56      0.98      0.71       310\n",
            "Personality disorder       1.00      0.23      0.37        22\n",
            "              Stress       1.00      0.10      0.18        50\n",
            "            Suicidal       0.60      0.40      0.48       204\n",
            "\n",
            "            accuracy                           0.57      1000\n",
            "           macro avg       0.79      0.38      0.42      1000\n",
            "        weighted avg       0.64      0.57      0.53      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming y_test are true labels and y_pred are predicted labels\n",
        "accuracy = accuracy_score(y_test, y_pred_best_bnb)  # Replace y_pred_best_bnb with your prediction variable\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Optional: print classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_best_bnb))\n",
        "\n",
        "print(\"\\n Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_best_bnb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8xyUZLWNefp",
        "outputId": "5503c845-856c-45c4-8696-e593efc54558"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.573\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "             Anxiety       0.86      0.32      0.47        75\n",
            "             Bipolar       1.00      0.09      0.17        55\n",
            "          Depression       0.54      0.52      0.53       284\n",
            "              Normal       0.56      0.98      0.71       310\n",
            "Personality disorder       1.00      0.23      0.37        22\n",
            "              Stress       1.00      0.10      0.18        50\n",
            "            Suicidal       0.60      0.40      0.48       204\n",
            "\n",
            "            accuracy                           0.57      1000\n",
            "           macro avg       0.79      0.38      0.42      1000\n",
            "        weighted avg       0.64      0.57      0.53      1000\n",
            "\n",
            "\n",
            " Confusion Matrix:\n",
            "[[ 24   0  14  34   0   0   3]\n",
            " [  0   5  36  14   0   0   0]\n",
            " [  2   0 148  89   0   0  45]\n",
            " [  0   0   4 304   0   0   2]\n",
            " [  0   0  11   6   5   0   0]\n",
            " [  2   0  13  26   0   5   4]\n",
            " [  0   0  50  72   0   0  82]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mentalHealth.py\n",
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import io\n",
        "import spacy\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "import nltk\n",
        "nltk.data.path.clear()  # Clear any cached paths\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Load spaCy model\n",
        "@st.cache_resource\n",
        "def load_spacy_model():\n",
        "    return spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = load_spacy_model()\n",
        "\n",
        "# Preprocessing function\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Original NLTK preprocessing\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Enhanced preprocessing with spaCy\n",
        "def enhanced_preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Process with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Combine spaCy and NLTK for optimal results\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct and len(token.text.strip()) > 0:\n",
        "            # Use spaCy's lemma but fall back to NLTK for pronouns\n",
        "            lemma = token.lemma_ if token.lemma_ != '-PRON-' else lemmatizer.lemmatize(token.text)\n",
        "            tokens.append(lemma)\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_texts(texts):\n",
        "    return [enhanced_preprocess_text(text) for text in texts]\n",
        "\n",
        "# Function for parallel processing\n",
        "def parallel_preprocessing(df, column_name):\n",
        "    num_cores = cpu_count()\n",
        "    df_split = np.array_split(df, num_cores)\n",
        "\n",
        "    # Create a multiprocessing Pool\n",
        "    with Pool(num_cores) as pool:\n",
        "        # Preprocess the text in parallel\n",
        "        results = pool.map(preprocess_texts, [batch[column_name].tolist() for batch in df_split])\n",
        "\n",
        "    # Combine the results\n",
        "    df['cleaned_statement'] = [item for sublist in results for item in sublist]\n",
        "    return df\n",
        "\n",
        "# OCR function to extract text from images\n",
        "def extract_text_from_image(image):\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "# Function to generate word cloud\n",
        "def generate_word_cloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400).generate(text)\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "    return fig\n",
        "\n",
        "# Function to extract mental health entities\n",
        "def extract_mental_health_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "\n",
        "    # Mental health related terms\n",
        "    mental_health_terms = [\"anxiety\", \"depression\", \"stress\", \"worried\", \"sad\",\n",
        "                          \"anxious\", \"panic\", \"fear\", \"mood\", \"emotion\"]\n",
        "\n",
        "    for token in doc:\n",
        "        if token.lemma_.lower() in mental_health_terms:\n",
        "            entities.append((token.text, \"MENTAL_CONDITION\"))\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Streamlit App Title\n",
        "st.title(\"Mental Health Prediction\")\n",
        "\n",
        "# Create tabs for different input methods\n",
        "tab1, tab2, tab3 = st.tabs([\"Upload CSV\", \"Image Input\", \"Text Input\"])\n",
        "\n",
        "with tab1:\n",
        "    # Upload File\n",
        "    uploaded_file = st.file_uploader(\"Upload CSV File\", type=[\"csv\"], key=\"csv_uploader\")\n",
        "    if uploaded_file:\n",
        "        df = pd.read_csv(uploaded_file, index_col=0, encoding='latin1')\n",
        "        # Limit the dataset to 5000 entries\n",
        "        # if len(df) > 5000:\n",
        "        #     df = df.sample(n=5000, random_state=42)\n",
        "\n",
        "        # Display data info\n",
        "        st.write(\"### Data Preview\")\n",
        "        st.write(df.head())\n",
        "\n",
        "        # Check for missing values\n",
        "        st.write(\"### Missing Values\")\n",
        "        st.write(df.isnull().sum())\n",
        "\n",
        "        # Drop missing values\n",
        "        df = df.dropna()\n",
        "        st.write(\"After dropping missing values:\", df.shape)\n",
        "\n",
        "        # EDA section\n",
        "        st.write(\"### Exploratory Data Analysis\")\n",
        "        st.write(df.info())\n",
        "        st.write(df.describe())\n",
        "\n",
        "        # Distribution of sentiments\n",
        "        sentiment_counts = df['status'].value_counts()\n",
        "        st.write(\"Sentiment Counts:\")\n",
        "        st.write(sentiment_counts)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        sentiment_counts.plot(kind='bar', title='Distribution of Sentiments', ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Calculate statement lengths\n",
        "        df['statement_length'] = df['statement'].apply(len)\n",
        "\n",
        "        # Statement length statistics before outlier removal\n",
        "        st.write(\"### Statement Length Statistics (Before Outlier Removal)\")\n",
        "        st.write(df['statement_length'].describe())\n",
        "\n",
        "        # Distribution of statement lengths before outlier removal\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)  # Ensure ax is bound to fig\n",
        "        df['statement_length'].hist(bins=100, ax=ax)\n",
        "        ax.set_title('Distribution of Statement Lengths (Before Outliers Removal)')\n",
        "        ax.set_xlabel('Length of Statements')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Compare statement lengths across categories using a boxplot before outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.boxplot(data=df, x='status', y='statement_length', palette='coolwarm', ax=ax)\n",
        "        ax.set_title('Statement Lengths by Mental Health Category (Before Outliers Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Violin plot before outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.violinplot(data=df, x='status', y='statement_length', palette='cool', split=True, inner=\"quartile\", ax=ax)\n",
        "        ax.set_title('Violin Plot of Statement Lengths Split by Category (Before Outliers Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Remove outliers based on IQR\n",
        "        Q1 = df['statement_length'].quantile(0.25)\n",
        "        Q3 = df['statement_length'].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        filtered_df = df[(df['statement_length'] >= lower_bound) & (df['statement_length'] <= upper_bound)]\n",
        "\n",
        "        st.write(f\"Original dataset size: {len(df)} rows\")\n",
        "        st.write(f\"After outlier removal: {len(filtered_df)} rows\")\n",
        "        st.write(f\"Removed {len(df) - len(filtered_df)} outliers\")\n",
        "\n",
        "        # Visualizations after outlier removal\n",
        "        st.write(\"### Visualizations After Outlier Removal\")\n",
        "\n",
        "        # Plot the distribution of statement lengths without outliers\n",
        "        fig, ax = plt.subplots()\n",
        "        filtered_df['statement_length'].hist(bins=100, ax=ax)\n",
        "        ax.set_title('Distribution of Statement Lengths (After Outliers Removal)')\n",
        "        ax.set_xlabel('Length of Statements')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Understand the frequency of each mental health category\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.countplot(data=filtered_df, x='status', palette='Set2', ax=ax)\n",
        "        ax.set_title('Distribution of Mental Health Categories')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Compare statement lengths across categories using a boxplot after outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.boxplot(data=filtered_df, x='status', y='statement_length', palette='coolwarm', ax=ax)\n",
        "        ax.set_title('Statement Lengths by Mental Health Category (After Outlier Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Violin plot after outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.violinplot(data=filtered_df, x='status', y='statement_length', palette='cool', inner=\"quartile\", ax=ax)\n",
        "        ax.set_title('Violin Plot of Statement Lengths by Mental Health Category (After Outlier Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Show the distribution of statement lengths by category\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        for category in filtered_df['status'].unique():\n",
        "            sns.kdeplot(filtered_df[filtered_df['status'] == category]['statement_length'], label=category, ax=ax)\n",
        "        ax.set_title('Density Plot of Statement Lengths by Category')\n",
        "        ax.set_xlabel('Statement Length')\n",
        "        ax.set_ylabel('Density')\n",
        "        ax.legend()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Visualize word frequency across categories using a heatmap\n",
        "        vectorizer = CountVectorizer(max_features=20, stop_words='english')\n",
        "        X = vectorizer.fit_transform(df['statement'])\n",
        "        word_freq_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        word_freq_df['status'] = df['status']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.heatmap(word_freq_df.groupby('status').mean(), cmap='YlGnBu', annot=True, ax=ax)\n",
        "        ax.set_title('Heatmap of Average Word Frequency by Mental Health Category')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Word clouds for each category\n",
        "        st.write(\"### Word Clouds by Category\")\n",
        "        for status in filtered_df['status'].unique():\n",
        "            status_text = ' '.join(filtered_df[filtered_df['status'] == status]['statement'])\n",
        "            fig = generate_word_cloud(status_text, title=f'Word Cloud for {status}')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        # Add a progress bar for feedback\n",
        "        progress_bar = st.progress(0)\n",
        "\n",
        "        # Choose preprocessing method\n",
        "        preprocessing_method = st.radio(\n",
        "            \"Select preprocessing method:\",\n",
        "            (\"NLTK (Original)\", \"spaCy Enhanced\", \"Combined (NLTK + spaCy)\")\n",
        "        )\n",
        "\n",
        "        # Add a confirm button\n",
        "        confirm_preprocessing = st.button(\"Confirm Preprocessing Method\")\n",
        "\n",
        "        # Only proceed with preprocessing when the confirm button is clicked\n",
        "        if confirm_preprocessing:\n",
        "            # Preprocessing\n",
        "            @st.cache_data\n",
        "            def preprocess_statements(df, method):\n",
        "                if method == \"NLTK (Original)\":\n",
        "                    df['cleaned_statement'] = df['statement'].apply(preprocess_text)\n",
        "                elif method == \"spaCy Enhanced\":\n",
        "                    st.info(\"Using spaCy for preprocessing (may take longer but provides better linguistic analysis)\")\n",
        "                    df = parallel_preprocessing(df, 'statement')\n",
        "                else:  # Combined\n",
        "                    st.info(\"Using combined NLTK and spaCy preprocessing\")\n",
        "                    df['cleaned_statement'] = df['statement'].apply(enhanced_preprocess_text)\n",
        "                return df\n",
        "            filtered_df = preprocess_statements(filtered_df, preprocessing_method)\n",
        "            progress_bar.progress(30)\n",
        "\n",
        "            # Train-Test Split\n",
        "            X = filtered_df['cleaned_statement']\n",
        "            y = filtered_df['status']\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "            # TF-IDF Vectorization\n",
        "            @st.cache_data\n",
        "            def vectorize_data(X_train, X_test):\n",
        "                vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=3000)\n",
        "                X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "                X_test_tfidf = vectorizer.transform(X_test)\n",
        "                return vectorizer, X_train_tfidf, X_test_tfidf\n",
        "\n",
        "            vectorizer, X_train_tfidf, X_test_tfidf = vectorize_data(X_train, X_test)\n",
        "            progress_bar.progress(60)\n",
        "\n",
        "            # SMOTE for Balancing\n",
        "            smote = SMOTE(random_state=0)\n",
        "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
        "            progress_bar.progress(80)\n",
        "\n",
        "            # Train SVM Model\n",
        "            svm_model = SVC(kernel='linear', C=1.0, probability=True)\n",
        "            svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "            progress_bar.progress(100)\n",
        "\n",
        "            # Classification Report\n",
        "            y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "            st.write(\"### Classification Report\")\n",
        "            st.text(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "            # Save model and vectorizer in session state\n",
        "            st.session_state['model'] = svm_model\n",
        "            st.session_state['vectorizer'] = vectorizer\n",
        "            st.session_state['classes'] = svm_model.classes_\n",
        "            st.session_state['preprocessing_method'] = preprocessing_method\n",
        "\n",
        "            st.success(f\"Model trained using {preprocessing_method} preprocessing method!\")\n",
        "        else :\n",
        "            st.info(\"Please select a preprocessing method and click 'Confirm Preprocessing Method' to proceed with training.\")\n",
        "\n",
        "with tab2:\n",
        "    st.write(\"### Upload an Image with Text\")\n",
        "    uploaded_image = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"], key=\"image_uploader\")\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Display the uploaded image\n",
        "        image = Image.open(uploaded_image)\n",
        "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "        # Extract text from the image\n",
        "        if st.button(\"Extract Text and Classify\"):\n",
        "            with st.spinner(\"Extracting text from image...\"):\n",
        "                extracted_text = extract_text_from_image(image)\n",
        "\n",
        "                if extracted_text.strip():\n",
        "                    st.write(\"### Extracted Text:\")\n",
        "                    st.write(extracted_text)\n",
        "\n",
        "                    # Check if model is trained\n",
        "                    if 'model' in st.session_state and 'vectorizer' in st.session_state:\n",
        "                        # Process and classify the extracted text\n",
        "                        if st.session_state.get('preprocessing_method') == \"spaCy Enhanced\":\n",
        "                            processed_text = enhanced_preprocess_text(extracted_text)\n",
        "                        elif st.session_state.get('preprocessing_method') == \"Combined (NLTK + spaCy)\":\n",
        "                            processed_text = enhanced_preprocess_text(extracted_text)\n",
        "                        else:  # NLTK (Original)\n",
        "                            processed_text = preprocess_text(extracted_text)\n",
        "\n",
        "\n",
        "                        vectorized_text = st.session_state['vectorizer'].transform([processed_text])\n",
        "                        prediction = st.session_state['model'].predict(vectorized_text)[0]\n",
        "                        probabilities = st.session_state['model'].predict_proba(vectorized_text)[0]\n",
        "\n",
        "                        st.write(\"### Classification Results:\")\n",
        "                        st.write(f\"*Predicted Category:* {prediction}\")\n",
        "                        st.write(\"Confidence Scores:\", dict(zip(st.session_state['classes'], probabilities)))\n",
        "\n",
        "                        # Extract mental health entities\n",
        "                        entities = extract_mental_health_entities(extracted_text)\n",
        "                        if entities:\n",
        "                            st.write(\"### Mental Health Entities Detected:\")\n",
        "                            for entity, label in entities:\n",
        "                                st.write(f\"- {entity} ({label})\")\n",
        "                        # Word cloud visualization for extracted text\n",
        "                        if len(extracted_text.split()) > 3:\n",
        "                            st.write(\"### Word Cloud:\")\n",
        "                            fig = generate_word_cloud(extracted_text, \"Extracted Text Word Cloud\")\n",
        "                            st.pyplot(fig)\n",
        "\n",
        "                    else:\n",
        "                        st.warning(\"Please upload a CSV file in the 'Upload CSV' tab first to train the model.\")\n",
        "                else:\n",
        "                    st.error(\"No text could be extracted from the image. Please try another image.\")\n",
        "\n",
        "with tab3:\n",
        "    st.write(\"### Predict Mental Health Category\")\n",
        "    user_input = st.text_area(\"Enter a statement:\")\n",
        "    if st.button(\"Classify\", key=\"classify_text\"):\n",
        "        if user_input:\n",
        "            if 'model' in st.session_state and 'vectorizer' in st.session_state:\n",
        "                # Use the same preprocessing method as training\n",
        "                if st.session_state.get('preprocessing_method') == \"spaCy Enhanced\":\n",
        "                    processed_text = enhanced_preprocess_text(user_input)\n",
        "                elif st.session_state.get('preprocessing_method') == \"Combined (NLTK + spaCy)\":\n",
        "                    processed_text = enhanced_preprocess_text(user_input)\n",
        "                else:  # NLTK (Original)\n",
        "                    processed_text = preprocess_text(user_input)\n",
        "\n",
        "                vectorized_text = st.session_state['vectorizer'].transform([processed_text])\n",
        "                prediction = st.session_state['model'].predict(vectorized_text)[0]\n",
        "                probabilities = st.session_state['model'].predict_proba(vectorized_text)[0]\n",
        "\n",
        "                st.write(f\"*Predicted Category:* {prediction}\")\n",
        "                st.write(\"Confidence Scores:\", dict(zip(st.session_state['classes'], probabilities)))\n",
        "\n",
        "                # Extract mental health entities\n",
        "                entities = extract_mental_health_entities(user_input)\n",
        "                if entities:\n",
        "                    st.write(\"### Mental Health Entities Detected:\")\n",
        "                    for entity, label in entities:\n",
        "                        st.write(f\"- {entity} ({label})\")\n",
        "                        # Word cloud visualization for user input\n",
        "                if len(user_input.split()) > 3:\n",
        "                    st.write(\"### Word Cloud:\")\n",
        "                    fig = generate_word_cloud(user_input, \"Input Text Word Cloud\")\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "            else:\n",
        "                st.warning(\"Please upload a CSV file in the 'Upload CSV' tab first to train the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9lsTUrh33Me",
        "outputId": "bccfa529-dff6-4bfa-9dfa-1beddac5f2c0"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mentalHealth.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!killall ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tdwlun6F6vd",
        "outputId": "99ddc180-0fc6-41a6-a0da-427dca6e6f5d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok: no process found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token\n",
        "ngrok.set_auth_token(\"2vNCVFg87afyBqt4Z8uJb5MhrDC_3HqemMVB9UZt4g7dEyyFM\")\n",
        "\n",
        "# Launch Streamlit in the background\n",
        "!streamlit run mentalHealth.py &>/dev/null &\n",
        "\n",
        "# Create tunnel and get public URL\n",
        "public_url = ngrok.connect(addr=8501)\n",
        "print(f\"🚀 Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tid2avMtF-z6",
        "outputId": "07445e7b-63fd-43d9-8ba5-68aa0529fd87"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Streamlit app is live at: NgrokTunnel: \"https://8efcb21eb89b.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_text = input(\" Enter a mental health-related statement: \")\n",
        "\n",
        "# Preprocess the input\n",
        "def preprocess_single_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "cleaned_input = preprocess_single_text(user_text)\n",
        "\n",
        "# Transform the input using the same TF-IDF vectorizer\n",
        "vectorized_input = vectoriser.transform([cleaned_input])\n",
        "\n",
        "# Predict using the trained model\n",
        "prediction = best_bnb.predict(vectorized_input)[0]\n",
        "\n",
        "print(f\"\\n🔮 Predicted Mental Health Category: **{prediction}**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRz_uP_sHnDb",
        "outputId": "afe120d9-e710-4e3c-ffc9-b0d2829269d7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enter a mental health-related statement: I am hoping it is soon. Before my birthday soon. I have completely given up on happiness, I have given up on trying to find friends, and I have given up on love. I have no reason to be here anymore, and I am tired of trying to convince myself otherwise. I am tired of false hope taunting me, always just out of my reach. Then, when I inevitable lose it, I end up feeling worse than before. It happens too often. I honestly might try filling my car with carbon monoxide tonight. I hear it can be peaceful. My life will end in suicide.\n",
            "\n",
            "🔮 Predicted Mental Health Category: **Suicidal**\n"
          ]
        }
      ]
    }
  ]
}