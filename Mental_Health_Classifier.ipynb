{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPodwJ//6WBdaiHqppp0W3s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejasvi1303/mentalhealthprediction/blob/main/Mental_Health_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk joblib streamlit pyngrok --quiet"
      ],
      "metadata": {
        "id": "-ZAYRl56XIqx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr -y\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3MeYsCGBgp4",
        "outputId": "a4dea4d2-8906-4882-eaa3-5038d40db675"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import lightgbm as lgb\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras import Model\n",
        "# Download necessary NLTK data files\n",
        "# Download necessary NLTK data files\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "file_path = '/content/Combined Data.csv'\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File found at:\", file_path)\n",
        "else:\n",
        "    print(\"File not found.\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"Combined Data.csv\", encoding='latin1', on_bad_lines='skip')\n",
        "df.head(10)\n",
        "df.tail(10)\n",
        "print(df.isnull().sum())\n",
        "df = df.dropna()\n",
        "print(df.isnull().sum())\n",
        "df['status'].nunique()\n",
        "sentiment_counts=df['status'].value_counts()\n",
        "print(sentiment_counts)\n",
        "df['status'].unique()\n",
        "df.shape\n",
        "df.info()\n",
        "# Calculate the length of each statement\n",
        "df['statement_length'] = df['statement'].apply(len)\n",
        "\n",
        "# Display basic statistics of statement lengths\n",
        "print(df['statement_length'].describe())\n",
        "\n",
        "# Taking a sample of the dataframe, e.g., 20,000 rows\n",
        "sample_size = 20000\n",
        "df_sample = df.sample(n=sample_size, random_state=1)\n",
        "# Load the spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "def preprocess_texts(texts):\n",
        "    return [preprocess_text(text) for text in texts]\n",
        "\n",
        "\n",
        "# Split the data into batches for multiprocessing\n",
        "num_cores = cpu_count()\n",
        "df_split = np.array_split(df_sample, num_cores)\n",
        "\n",
        "# Create a multiprocessing Pool\n",
        "with Pool(num_cores) as pool:\n",
        "    # Preprocess the text in parallel\n",
        "    results = pool.map(preprocess_texts, [batch['statement'].tolist() for batch in df_split])\n",
        "\n",
        "# Combine the results\n",
        "df_sample['cleaned_statement'] = [item for sublist in results for item in sublist]\n",
        "\n",
        "# Display the first few rows of the DataFrame to confirm the changes\n",
        "print(df_sample[['statement', 'cleaned_statement']].head())\n",
        "df_sample.head()\n",
        "# Extract features and labels\n",
        "processedtext = df_sample['cleaned_statement']\n",
        "sentiment = df_sample['status']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
        "                                                    test_size=0.05, random_state=0)\n",
        "\n",
        "print(f'X_train size: {len(X_train)}')\n",
        "print(f'X_test size: {len(X_test)}')\n",
        "print(f'y_train size: {len(y_train)}')\n",
        "print(f'y_test size: {len(y_test)}')\n",
        "\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "vectoriser.fit(X_train)\n",
        "print(f'Vectoriser fitted.')\n",
        "print('No. of feature_words: ', len(vectoriser.get_feature_names_out()))\n",
        "\n",
        "X_train = vectoriser.transform(X_train)\n",
        "X_test  = vectoriser.transform(X_test)\n",
        "\n",
        "# Use SMOTE to oversample the training data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=bnb, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Score: {grid_search.best_score_}')\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_bnb = grid_search.best_estimator_\n",
        "best_bnb.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_best_bnb = best_bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Tuned Bernoulli Naive Bayes\")\n",
        "print(classification_report(y_test, y_pred_best_bnb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w6gZkH7IYeL",
        "outputId": "6d11fa59-cc11-4022-bcb1-5f373cca18a6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found at: /content/Combined Data.csv\n",
            "Unnamed: 0      0\n",
            "statement     362\n",
            "status          0\n",
            "dtype: int64\n",
            "Unnamed: 0    0\n",
            "statement     0\n",
            "status        0\n",
            "dtype: int64\n",
            "status\n",
            "Normal                  16343\n",
            "Depression              15404\n",
            "Suicidal                10652\n",
            "Anxiety                  3841\n",
            "Bipolar                  2777\n",
            "Stress                   2587\n",
            "Personality disorder     1077\n",
            "Name: count, dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 52681 entries, 0 to 53042\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  52681 non-null  int64 \n",
            " 1   statement   52681 non-null  object\n",
            " 2   status      52681 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.6+ MB\n",
            "count    52681.000000\n",
            "mean       578.718153\n",
            "std        846.274948\n",
            "min          2.000000\n",
            "25%         80.000000\n",
            "50%        317.000000\n",
            "75%        752.000000\n",
            "max      32759.000000\n",
            "Name: statement_length, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               statement  \\\n",
            "39488  i m and i have bad anxiety debilitating i have...   \n",
            "1480   Jessica starred in the musical \"Legally Blonde...   \n",
            "47948  IÂ’m so tired I just donÂ’t see a point to my su...   \n",
            "19681  My life 1 year ago was completely different. I...   \n",
            "37629  RT @no_onespecixl: Know one enjoys my company ...   \n",
            "\n",
            "                                       cleaned_statement  \n",
            "39488  bad anxiety debilitating able keep job since w...  \n",
            "1480   jessica starred musical legally blonde elle wo...  \n",
            "47948  iÂ’m tired donÂ’t see point suffering donÂ’t unde...  \n",
            "19681  life 1 year ago completely different chick mag...  \n",
            "37629  rt noonespecixl know one enjoys company make e...  \n",
            "X_train size: 19000\n",
            "X_test size: 1000\n",
            "y_train size: 19000\n",
            "y_test size: 1000\n",
            "Vectoriser fitted.\n",
            "No. of feature_words:  500000\n",
            "Best Parameters: {'alpha': 0.1}\n",
            "Best Score: 0.8011026708183809\n",
            "Tuned Bernoulli Naive Bayes\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "             Anxiety       0.86      0.32      0.47        75\n",
            "             Bipolar       1.00      0.09      0.17        55\n",
            "          Depression       0.54      0.52      0.53       284\n",
            "              Normal       0.56      0.98      0.71       310\n",
            "Personality disorder       1.00      0.23      0.37        22\n",
            "              Stress       1.00      0.10      0.18        50\n",
            "            Suicidal       0.60      0.40      0.48       204\n",
            "\n",
            "            accuracy                           0.57      1000\n",
            "           macro avg       0.79      0.38      0.42      1000\n",
            "        weighted avg       0.64      0.57      0.53      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming y_test are true labels and y_pred are predicted labels\n",
        "accuracy = accuracy_score(y_test, y_pred_best_bnb)  # Replace y_pred_best_bnb with your prediction variable\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Optional: print classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_best_bnb))\n",
        "\n",
        "print(\"\\n Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_best_bnb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8xyUZLWNefp",
        "outputId": "5503c845-856c-45c4-8696-e593efc54558"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.573\n",
            "\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "             Anxiety       0.86      0.32      0.47        75\n",
            "             Bipolar       1.00      0.09      0.17        55\n",
            "          Depression       0.54      0.52      0.53       284\n",
            "              Normal       0.56      0.98      0.71       310\n",
            "Personality disorder       1.00      0.23      0.37        22\n",
            "              Stress       1.00      0.10      0.18        50\n",
            "            Suicidal       0.60      0.40      0.48       204\n",
            "\n",
            "            accuracy                           0.57      1000\n",
            "           macro avg       0.79      0.38      0.42      1000\n",
            "        weighted avg       0.64      0.57      0.53      1000\n",
            "\n",
            "\n",
            " Confusion Matrix:\n",
            "[[ 24   0  14  34   0   0   3]\n",
            " [  0   5  36  14   0   0   0]\n",
            " [  2   0 148  89   0   0  45]\n",
            " [  0   0   4 304   0   0   2]\n",
            " [  0   0  11   6   5   0   0]\n",
            " [  2   0  13  26   0   5   4]\n",
            " [  0   0  50  72   0   0  82]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mentalHealth.py\n",
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import io\n",
        "import spacy\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "import nltk\n",
        "nltk.data.path.clear()  # Clear any cached paths\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Load spaCy model\n",
        "@st.cache_resource\n",
        "def load_spacy_model():\n",
        "    return spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = load_spacy_model()\n",
        "\n",
        "# Preprocessing function\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Original NLTK preprocessing\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Enhanced preprocessing with spaCy\n",
        "def enhanced_preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Process with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Combine spaCy and NLTK for optimal results\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct and len(token.text.strip()) > 0:\n",
        "            # Use spaCy's lemma but fall back to NLTK for pronouns\n",
        "            lemma = token.lemma_ if token.lemma_ != '-PRON-' else lemmatizer.lemmatize(token.text)\n",
        "            tokens.append(lemma)\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_texts(texts):\n",
        "    return [enhanced_preprocess_text(text) for text in texts]\n",
        "\n",
        "# Function for parallel processing\n",
        "def parallel_preprocessing(df, column_name):\n",
        "    num_cores = cpu_count()\n",
        "    df_split = np.array_split(df, num_cores)\n",
        "\n",
        "    # Create a multiprocessing Pool\n",
        "    with Pool(num_cores) as pool:\n",
        "        # Preprocess the text in parallel\n",
        "        results = pool.map(preprocess_texts, [batch[column_name].tolist() for batch in df_split])\n",
        "\n",
        "    # Combine the results\n",
        "    df['cleaned_statement'] = [item for sublist in results for item in sublist]\n",
        "    return df\n",
        "\n",
        "# OCR function to extract text from images\n",
        "def extract_text_from_image(image):\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "# Function to generate word cloud\n",
        "def generate_word_cloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400).generate(text)\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "    return fig\n",
        "\n",
        "# Function to extract mental health entities\n",
        "def extract_mental_health_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "\n",
        "    # Mental health related terms\n",
        "    mental_health_terms = [\"anxiety\", \"depression\", \"stress\", \"worried\", \"sad\",\n",
        "                          \"anxious\", \"panic\", \"fear\", \"mood\", \"emotion\"]\n",
        "\n",
        "    for token in doc:\n",
        "        if token.lemma_.lower() in mental_health_terms:\n",
        "            entities.append((token.text, \"MENTAL_CONDITION\"))\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Streamlit App Title\n",
        "st.title(\"Mental Health Prediction\")\n",
        "\n",
        "# Create tabs for different input methods\n",
        "tab1, tab2, tab3 = st.tabs([\"Upload CSV\", \"Image Input\", \"Text Input\"])\n",
        "\n",
        "with tab1:\n",
        "    # Upload File\n",
        "    uploaded_file = st.file_uploader(\"Upload CSV File\", type=[\"csv\"], key=\"csv_uploader\")\n",
        "    if uploaded_file:\n",
        "        df = pd.read_csv(uploaded_file, index_col=0, encoding='latin1')\n",
        "        # Limit the dataset to 5000 entries\n",
        "        # if len(df) > 5000:\n",
        "        #     df = df.sample(n=5000, random_state=42)\n",
        "\n",
        "        # Display data info\n",
        "        st.write(\"### Data Preview\")\n",
        "        st.write(df.head())\n",
        "\n",
        "        # Check for missing values\n",
        "        st.write(\"### Missing Values\")\n",
        "        st.write(df.isnull().sum())\n",
        "\n",
        "        # Drop missing values\n",
        "        df = df.dropna()\n",
        "        st.write(\"After dropping missing values:\", df.shape)\n",
        "\n",
        "        # EDA section\n",
        "        st.write(\"### Exploratory Data Analysis\")\n",
        "        st.write(df.info())\n",
        "        st.write(df.describe())\n",
        "\n",
        "        # Distribution of sentiments\n",
        "        sentiment_counts = df['status'].value_counts()\n",
        "        st.write(\"Sentiment Counts:\")\n",
        "        st.write(sentiment_counts)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        sentiment_counts.plot(kind='bar', title='Distribution of Sentiments', ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Calculate statement lengths\n",
        "        df['statement_length'] = df['statement'].apply(len)\n",
        "\n",
        "        # Statement length statistics before outlier removal\n",
        "        st.write(\"### Statement Length Statistics (Before Outlier Removal)\")\n",
        "        st.write(df['statement_length'].describe())\n",
        "\n",
        "        # Distribution of statement lengths before outlier removal\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)  # Ensure ax is bound to fig\n",
        "        df['statement_length'].hist(bins=100, ax=ax)\n",
        "        ax.set_title('Distribution of Statement Lengths (Before Outliers Removal)')\n",
        "        ax.set_xlabel('Length of Statements')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Compare statement lengths across categories using a boxplot before outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.boxplot(data=df, x='status', y='statement_length', palette='coolwarm', ax=ax)\n",
        "        ax.set_title('Statement Lengths by Mental Health Category (Before Outliers Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Violin plot before outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.violinplot(data=df, x='status', y='statement_length', palette='cool', split=True, inner=\"quartile\", ax=ax)\n",
        "        ax.set_title('Violin Plot of Statement Lengths Split by Category (Before Outliers Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Remove outliers based on IQR\n",
        "        Q1 = df['statement_length'].quantile(0.25)\n",
        "        Q3 = df['statement_length'].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        filtered_df = df[(df['statement_length'] >= lower_bound) & (df['statement_length'] <= upper_bound)]\n",
        "\n",
        "        st.write(f\"Original dataset size: {len(df)} rows\")\n",
        "        st.write(f\"After outlier removal: {len(filtered_df)} rows\")\n",
        "        st.write(f\"Removed {len(df) - len(filtered_df)} outliers\")\n",
        "\n",
        "        # Visualizations after outlier removal\n",
        "        st.write(\"### Visualizations After Outlier Removal\")\n",
        "\n",
        "        # Plot the distribution of statement lengths without outliers\n",
        "        fig, ax = plt.subplots()\n",
        "        filtered_df['statement_length'].hist(bins=100, ax=ax)\n",
        "        ax.set_title('Distribution of Statement Lengths (After Outliers Removal)')\n",
        "        ax.set_xlabel('Length of Statements')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Understand the frequency of each mental health category\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.countplot(data=filtered_df, x='status', palette='Set2', ax=ax)\n",
        "        ax.set_title('Distribution of Mental Health Categories')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Compare statement lengths across categories using a boxplot after outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.boxplot(data=filtered_df, x='status', y='statement_length', palette='coolwarm', ax=ax)\n",
        "        ax.set_title('Statement Lengths by Mental Health Category (After Outlier Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Violin plot after outlier removal\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.violinplot(data=filtered_df, x='status', y='statement_length', palette='cool', inner=\"quartile\", ax=ax)\n",
        "        ax.set_title('Violin Plot of Statement Lengths by Mental Health Category (After Outlier Removal)')\n",
        "        ax.set_xlabel('Mental Health Category')\n",
        "        ax.set_ylabel('Statement Length')\n",
        "        plt.xticks(rotation=45)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Show the distribution of statement lengths by category\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        for category in filtered_df['status'].unique():\n",
        "            sns.kdeplot(filtered_df[filtered_df['status'] == category]['statement_length'], label=category, ax=ax)\n",
        "        ax.set_title('Density Plot of Statement Lengths by Category')\n",
        "        ax.set_xlabel('Statement Length')\n",
        "        ax.set_ylabel('Density')\n",
        "        ax.legend()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Visualize word frequency across categories using a heatmap\n",
        "        vectorizer = CountVectorizer(max_features=20, stop_words='english')\n",
        "        X = vectorizer.fit_transform(df['statement'])\n",
        "        word_freq_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        word_freq_df['status'] = df['status']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.heatmap(word_freq_df.groupby('status').mean(), cmap='YlGnBu', annot=True, ax=ax)\n",
        "        ax.set_title('Heatmap of Average Word Frequency by Mental Health Category')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Word clouds for each category\n",
        "        st.write(\"### Word Clouds by Category\")\n",
        "        for status in filtered_df['status'].unique():\n",
        "            status_text = ' '.join(filtered_df[filtered_df['status'] == status]['statement'])\n",
        "            fig = generate_word_cloud(status_text, title=f'Word Cloud for {status}')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        # Add a progress bar for feedback\n",
        "        progress_bar = st.progress(0)\n",
        "\n",
        "        # Choose preprocessing method\n",
        "        preprocessing_method = st.radio(\n",
        "            \"Select preprocessing method:\",\n",
        "            (\"NLTK (Original)\", \"spaCy Enhanced\", \"Combined (NLTK + spaCy)\")\n",
        "        )\n",
        "\n",
        "        # Add a confirm button\n",
        "        confirm_preprocessing = st.button(\"Confirm Preprocessing Method\")\n",
        "\n",
        "        # Only proceed with preprocessing when the confirm button is clicked\n",
        "        if confirm_preprocessing:\n",
        "            # Preprocessing\n",
        "            @st.cache_data\n",
        "            def preprocess_statements(df, method):\n",
        "                if method == \"NLTK (Original)\":\n",
        "                    df['cleaned_statement'] = df['statement'].apply(preprocess_text)\n",
        "                elif method == \"spaCy Enhanced\":\n",
        "                    st.info(\"Using spaCy for preprocessing (may take longer but provides better linguistic analysis)\")\n",
        "                    df = parallel_preprocessing(df, 'statement')\n",
        "                else:  # Combined\n",
        "                    st.info(\"Using combined NLTK and spaCy preprocessing\")\n",
        "                    df['cleaned_statement'] = df['statement'].apply(enhanced_preprocess_text)\n",
        "                return df\n",
        "            filtered_df = preprocess_statements(filtered_df, preprocessing_method)\n",
        "            progress_bar.progress(30)\n",
        "\n",
        "            # Train-Test Split\n",
        "            X = filtered_df['cleaned_statement']\n",
        "            y = filtered_df['status']\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "            # TF-IDF Vectorization\n",
        "            @st.cache_data\n",
        "            def vectorize_data(X_train, X_test):\n",
        "                vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=3000)\n",
        "                X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "                X_test_tfidf = vectorizer.transform(X_test)\n",
        "                return vectorizer, X_train_tfidf, X_test_tfidf\n",
        "\n",
        "            vectorizer, X_train_tfidf, X_test_tfidf = vectorize_data(X_train, X_test)\n",
        "            progress_bar.progress(60)\n",
        "\n",
        "            # SMOTE for Balancing\n",
        "            smote = SMOTE(random_state=0)\n",
        "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
        "            progress_bar.progress(80)\n",
        "\n",
        "            # Train SVM Model\n",
        "            svm_model = SVC(kernel='linear', C=1.0, probability=True)\n",
        "            svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "            progress_bar.progress(100)\n",
        "\n",
        "            # Classification Report\n",
        "            y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "            st.write(\"### Classification Report\")\n",
        "            st.text(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "            # Save model and vectorizer in session state\n",
        "            st.session_state['model'] = svm_model\n",
        "            st.session_state['vectorizer'] = vectorizer\n",
        "            st.session_state['classes'] = svm_model.classes_\n",
        "            st.session_state['preprocessing_method'] = preprocessing_method\n",
        "\n",
        "            st.success(f\"Model trained using {preprocessing_method} preprocessing method!\")\n",
        "        else :\n",
        "            st.info(\"Please select a preprocessing method and click 'Confirm Preprocessing Method' to proceed with training.\")\n",
        "\n",
        "with tab2:\n",
        "    st.write(\"### Upload an Image with Text\")\n",
        "    uploaded_image = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"], key=\"image_uploader\")\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Display the uploaded image\n",
        "        image = Image.open(uploaded_image)\n",
        "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "        # Extract text from the image\n",
        "        if st.button(\"Extract Text and Classify\"):\n",
        "            with st.spinner(\"Extracting text from image...\"):\n",
        "                extracted_text = extract_text_from_image(image)\n",
        "\n",
        "                if extracted_text.strip():\n",
        "                    st.write(\"### Extracted Text:\")\n",
        "                    st.write(extracted_text)\n",
        "\n",
        "                    # Check if model is trained\n",
        "                    if 'model' in st.session_state and 'vectorizer' in st.session_state:\n",
        "                        # Process and classify the extracted text\n",
        "                        if st.session_state.get('preprocessing_method') == \"spaCy Enhanced\":\n",
        "                            processed_text = enhanced_preprocess_text(extracted_text)\n",
        "                        elif st.session_state.get('preprocessing_method') == \"Combined (NLTK + spaCy)\":\n",
        "                            processed_text = enhanced_preprocess_text(extracted_text)\n",
        "                        else:  # NLTK (Original)\n",
        "                            processed_text = preprocess_text(extracted_text)\n",
        "\n",
        "\n",
        "                        vectorized_text = st.session_state['vectorizer'].transform([processed_text])\n",
        "                        prediction = st.session_state['model'].predict(vectorized_text)[0]\n",
        "                        probabilities = st.session_state['model'].predict_proba(vectorized_text)[0]\n",
        "\n",
        "                        st.write(\"### Classification Results:\")\n",
        "                        st.write(f\"*Predicted Category:* {prediction}\")\n",
        "                        st.write(\"Confidence Scores:\", dict(zip(st.session_state['classes'], probabilities)))\n",
        "\n",
        "                        # Extract mental health entities\n",
        "                        entities = extract_mental_health_entities(extracted_text)\n",
        "                        if entities:\n",
        "                            st.write(\"### Mental Health Entities Detected:\")\n",
        "                            for entity, label in entities:\n",
        "                                st.write(f\"- {entity} ({label})\")\n",
        "                        # Word cloud visualization for extracted text\n",
        "                        if len(extracted_text.split()) > 3:\n",
        "                            st.write(\"### Word Cloud:\")\n",
        "                            fig = generate_word_cloud(extracted_text, \"Extracted Text Word Cloud\")\n",
        "                            st.pyplot(fig)\n",
        "\n",
        "                    else:\n",
        "                        st.warning(\"Please upload a CSV file in the 'Upload CSV' tab first to train the model.\")\n",
        "                else:\n",
        "                    st.error(\"No text could be extracted from the image. Please try another image.\")\n",
        "\n",
        "with tab3:\n",
        "    st.write(\"### Predict Mental Health Category\")\n",
        "    user_input = st.text_area(\"Enter a statement:\")\n",
        "    if st.button(\"Classify\", key=\"classify_text\"):\n",
        "        if user_input:\n",
        "            if 'model' in st.session_state and 'vectorizer' in st.session_state:\n",
        "                # Use the same preprocessing method as training\n",
        "                if st.session_state.get('preprocessing_method') == \"spaCy Enhanced\":\n",
        "                    processed_text = enhanced_preprocess_text(user_input)\n",
        "                elif st.session_state.get('preprocessing_method') == \"Combined (NLTK + spaCy)\":\n",
        "                    processed_text = enhanced_preprocess_text(user_input)\n",
        "                else:  # NLTK (Original)\n",
        "                    processed_text = preprocess_text(user_input)\n",
        "\n",
        "                vectorized_text = st.session_state['vectorizer'].transform([processed_text])\n",
        "                prediction = st.session_state['model'].predict(vectorized_text)[0]\n",
        "                probabilities = st.session_state['model'].predict_proba(vectorized_text)[0]\n",
        "\n",
        "                st.write(f\"*Predicted Category:* {prediction}\")\n",
        "                st.write(\"Confidence Scores:\", dict(zip(st.session_state['classes'], probabilities)))\n",
        "\n",
        "                # Extract mental health entities\n",
        "                entities = extract_mental_health_entities(user_input)\n",
        "                if entities:\n",
        "                    st.write(\"### Mental Health Entities Detected:\")\n",
        "                    for entity, label in entities:\n",
        "                        st.write(f\"- {entity} ({label})\")\n",
        "                        # Word cloud visualization for user input\n",
        "                if len(user_input.split()) > 3:\n",
        "                    st.write(\"### Word Cloud:\")\n",
        "                    fig = generate_word_cloud(user_input, \"Input Text Word Cloud\")\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "            else:\n",
        "                st.warning(\"Please upload a CSV file in the 'Upload CSV' tab first to train the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9lsTUrh33Me",
        "outputId": "bccfa529-dff6-4bfa-9dfa-1beddac5f2c0"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mentalHealth.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!killall ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tdwlun6F6vd",
        "outputId": "99ddc180-0fc6-41a6-a0da-427dca6e6f5d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok: no process found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token\n",
        "ngrok.set_auth_token(\"2vNCVFg87afyBqt4Z8uJb5MhrDC_3HqemMVB9UZt4g7dEyyFM\")\n",
        "\n",
        "# Launch Streamlit in the background\n",
        "!streamlit run mentalHealth.py &>/dev/null &\n",
        "\n",
        "# Create tunnel and get public URL\n",
        "public_url = ngrok.connect(addr=8501)\n",
        "print(f\"ðŸš€ Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tid2avMtF-z6",
        "outputId": "07445e7b-63fd-43d9-8ba5-68aa0529fd87"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Streamlit app is live at: NgrokTunnel: \"https://8efcb21eb89b.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_text = input(\" Enter a mental health-related statement: \")\n",
        "\n",
        "# Preprocess the input\n",
        "def preprocess_single_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "cleaned_input = preprocess_single_text(user_text)\n",
        "\n",
        "# Transform the input using the same TF-IDF vectorizer\n",
        "vectorized_input = vectoriser.transform([cleaned_input])\n",
        "\n",
        "# Predict using the trained model\n",
        "prediction = best_bnb.predict(vectorized_input)[0]\n",
        "\n",
        "print(f\"\\nðŸ”® Predicted Mental Health Category: **{prediction}**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRz_uP_sHnDb",
        "outputId": "afe120d9-e710-4e3c-ffc9-b0d2829269d7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enter a mental health-related statement: I am hoping it is soon. Before my birthday soon. I have completely given up on happiness, I have given up on trying to find friends, and I have given up on love. I have no reason to be here anymore, and I am tired of trying to convince myself otherwise. I am tired of false hope taunting me, always just out of my reach. Then, when I inevitable lose it, I end up feeling worse than before. It happens too often. I honestly might try filling my car with carbon monoxide tonight. I hear it can be peaceful. My life will end in suicide.\n",
            "\n",
            "ðŸ”® Predicted Mental Health Category: **Suicidal**\n"
          ]
        }
      ]
    }
  ]
}